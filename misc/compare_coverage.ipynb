{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of coverage\n",
    "\n",
    "The paper, \"Confidence limits for prevalence of disease adjustedfor estimated sensitivity and specificity\" by Lang and Reiczigel propose an approximate approach to produce confidence intervals.\n",
    "\n",
    "In their table 1, they show that for true specificity 0.99 and true prevalence near 0.02, the method produces 95% confidence intervals that significantly overcover (coverage is about 99.4%). Since this is the domain pertenant to SARS-CoV-2 sero-prevalence, it is possible that this approach produces confidence intervals that are larger than necessary.\n",
    "\n",
    "In the following, I confirm the coverage calculation using the calculator:\n",
    "http://www2.univet.hu/users/jreiczig/CI4prevSeSp/calc02/index.php\n",
    "\n",
    " * True sensitivity = 0.99\n",
    " * True specificity = 0.99\n",
    " * n_se = 100\n",
    " * n_sp = 100\n",
    "\n",
    " * True prevalence = 0.02\n",
    "\n",
    " * n_p = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9940491846455418\n",
      "(Table 1 shows this value as 0.994)\n"
     ]
    }
   ],
   "source": [
    "# Lang and Reiczigel method:\n",
    "# online calculator produces 95% CL intervals that contain the true prevalence (0.02) for indicated range of positives\n",
    "# (indexed by the number of false positives) - it does not depend on the observed number of false negatives\n",
    "ranges = {\n",
    "    0:[0,7],\n",
    "    1:[0,8],\n",
    "    2:[0,10],\n",
    "    3:[0,12],\n",
    "    4:[1,13],\n",
    "    5:[1,15],\n",
    "    6:[2,16]\n",
    "}\n",
    "\n",
    "n_p = 100\n",
    "n_sp = 100\n",
    "\n",
    "# probability of a positive = true prevalence*sensitivity + false positive rate\n",
    "prev = 0.02\n",
    "sensitivity = 0.99\n",
    "p_fp = 0.01\n",
    "p = prev*sensitivity + p_fp\n",
    "\n",
    "# consider the possible outcomes - tally the probability that the true prevalence is in the reported 95% CL intervals.\n",
    "\n",
    "prob_in_interval = 0.\n",
    "\n",
    "for k_fp in range(7):\n",
    "    # probability that the specificity test results in k_fp false positives\n",
    "    prob_kfp = stats.binom.pmf(k_fp,n_sp,p_fp)\n",
    "    \n",
    "    # work out probability, given k_fp, that the number of positives will produce CI containing true prevalence\n",
    "    upper = ranges[k_fp][1]\n",
    "    prob_kp = stats.binom.cdf(upper,n_p,p)\n",
    "    lower = ranges[k_fp][0]\n",
    "    if lower > 0:\n",
    "        prob_kp -= stats.binom.cdf(lower-1,n_p,p)\n",
    "        \n",
    "    # joint probability for k_fp and CI contains true prevalence:\n",
    "    product = prob_kfp * prob_kp\n",
    "    prob_in_interval += product\n",
    "    \n",
    "print(prob_in_interval)\n",
    "print('(Table 1 shows this value as 0.994)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with approach based on Bayesian inference of false positive rate\n",
    "\n",
    "Prior density for false positive rate is assumed to be uniform.\n",
    "\n",
    "Given a value for k_fp (the number of negative samples falsely identified as positive) the posterior probability density for the false positive probability (eta) is proporational to the likelihood(eta) = binom.pmf(k_fp, n_fp, eta).\n",
    "\n",
    "Use that posterior to represent the probability distribution for the false positive rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw a fpr from the posterior distribution (acceptance rejection)\n",
    "def get_fpr(i_fp, i_control):\n",
    "    nu = 1.*i_fp/i_control\n",
    "    big = stats.binom.pmf(i_fp, i_control, nu)\n",
    "    max_fpr = 0.2 # fpr will be restricted to be below this value\n",
    "    fpr_result = -1.\n",
    "    while fpr_result < 0.:\n",
    "        fpr_trial = stats.uniform.rvs(0.,max_fpr)\n",
    "        pdf = stats.binom.pmf(i_fp, i_control, fpr_trial)\n",
    "        if stats.uniform.rvs(0.,big) < pdf:\n",
    "            fpr_result = fpr_trial\n",
    "    return fpr_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fp low high\n",
      "0 0 8\n",
      "1 0 10\n",
      "2 1 11\n",
      "3 1 14\n",
      "4 2 14\n",
      "5 2 16\n",
      "6 3 18\n",
      "coverage:  0.9830613479681555\n"
     ]
    }
   ],
   "source": [
    "prev = 0.02\n",
    "p = prev*sensitivity + p_fp\n",
    "\n",
    "cl = 0.95\n",
    "upper = 1.-(1-cl)/2.\n",
    "lower = (1-cl)/2.\n",
    "\n",
    "n_reps = 1000\n",
    "i_upper = int(upper*n_reps)\n",
    "i_lower = int(lower*n_reps)\n",
    "\n",
    "prob_in_interval = 0.\n",
    "\n",
    "print('fp low high')\n",
    "\n",
    "for k_fp in range(7):\n",
    "    # probability that the specificity test results in k_fp false positives\n",
    "    prob_kfp = stats.binom.pmf(k_fp,n_sp,p_fp)\n",
    "\n",
    "    # work out the distribution of observations for k_p\n",
    "    results = []\n",
    "    for i in range(n_reps):\n",
    "        fpr = get_fpr(k_fp, n_sp)\n",
    "        prob = prev*sensitivity + fpr\n",
    "        results.append(stats.binom.rvs(n_p, prob))\n",
    "        \n",
    "    results.sort()\n",
    "    upper = results[i_upper]\n",
    "    lower = results[i_lower]\n",
    "    print(k_fp,lower,upper)\n",
    "    \n",
    "    prob_kp = stats.binom.cdf(upper,n_p,p)\n",
    "    if lower > 0:\n",
    "        prob_kp -= stats.binom.cdf(lower-1,n_p,p)\n",
    "        \n",
    "    # joint probability for k_fp and CI contains true prevalence:\n",
    "    product = prob_kfp * prob_kp\n",
    "    prob_in_interval += product\n",
    "    \n",
    "print('coverage: ',prob_in_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings\n",
    "\n",
    "Both methods overcover. The overcoverage of the L&R method is more severe. The tables shows for different prevalences (prev) the coverage for the 95% CL intervals by the two methods considered.\n",
    "\n",
    "prev | L&R | Bayes\n",
    "---|---|---\n",
    "0.005 | 0.994 | 0.994\n",
    "0.01 | 0.994 | 0.996\n",
    "0.02 | 0.994 | 0.983\n",
    "0.03 | 0.992 | 0.974\n",
    "0.05 | 0.984 | 0.972"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
